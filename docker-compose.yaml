version: '3.8'
services:
  dag-syncer:
      image: minio/mc:latest
      container_name: airflow-dag-sync
      volumes:
        - airflow-dag-storage:/dags
      entrypoint: >
        /bin/sh -c "
        # 1. Connect to your MinIO instance (Update IP/Credentials if needed)
        mc alias set myminio http://144.24.127.112:9000 minioadmin minioadmin;
        # 2. Ensure the bucket exists
        mc mb --ignore-existing myminio/airflow-dags;
        # 3. Mirror MinIO to local volume and watch for changes (--watch)
        mc mirror --watch --overwrite myminio/airflow-dags /dags;
        "
  airflow-webserver:
    build: .
    container_name: airflow-server
    env_file:
      - .env
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:2324@144.24.127.112:5432/airflow_db
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - _AIRFLOW_WWW_USER_CREATE=false
      - PYTHONUNBUFFERED=1
      # --- ADD THIS FOR AIRFLOW 3 LDAP/FAB ---
      - AIRFLOW__CORE__AUTH_MANAGER=airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./webserver_config.py:/opt/airflow/webserver_config.py
    ports:
      - "8080:8080"
    command: api-server
    # Added healthcheck so the scheduler knows when the webserver is ready
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/webapp/health"]
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    build: .
    container_name: airflow-scheduler
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:2324@144.24.127.112:5432/airflow_db
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
    # CHANGE HERE: Wait for the webserver to finish migrations
    depends_on:
      airflow-webserver:
        condition: service_healthy
    command: scheduler